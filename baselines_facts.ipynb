{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports and datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from utils import utils_vectorize as uv\n",
    "\n",
    "with open('datasets/msrpar_samples.pkl', 'rb') as f: \n",
    "    par_samples, par_scores = pickle.load(f)\n",
    "with open('datasets/msrvid_samples.pkl', 'rb') as f: \n",
    "    vid_samples, vid_scores = pickle.load(f)\n",
    "with open('datasets/msranswer_samples.pkl', 'rb') as f: \n",
    "    answer_samples, answer_scores = pickle.load(f)\n",
    "# DEF2DEF flavors: original, adjusted, and 250 adjusted samples\n",
    "with open('datasets/def2def_samples.pkl', 'rb') as f: \n",
    "    def2def_samples, def2def_scores = pickle.load(f)\n",
    "with open('datasets/def2def_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def_adj_samples, def2def_adj_scores = pickle.load(f)\n",
    "with open('datasets/def2def250_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def250_samples, def2def250_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinations: composition = (sum | ICDS); sim = (cos | ICM | euclid | dot prod)\n",
    "# Receives a fact/triplet and returns a representative embedding, including subject, relation and object embeddings\n",
    "def txtfact_to_vector(fact:tuple, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    # 1. A sequential composition into each element of triplet\n",
    "    v_subj = uv.icds_vectorize(fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) #* 1.\n",
    "    v_rel = uv.icds_vectorize(fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) #* 1\n",
    "    v_obj = uv.icds_vectorize(fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * 3 #* 2 \n",
    "    # 2. A last composition (see other composition functions) of the whole fact\n",
    "    v_fact = uv.icds_composition(v_rel, v_obj)\n",
    "    v_fact = uv.icds_composition(v_fact, v_subj)   # Best composition order\n",
    "    return(v_fact, v_subj, v_rel, v_obj)\n",
    "\n",
    "# Receives a complete knowledge graph (representative of a sentence) and returns a single vector\n",
    "def txtkg_to_vector(txt_kg, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    n_facts = len(txt_kg)\n",
    "    kg_vectors = []\n",
    "    for txt_fact in txt_kg:\n",
    "        v_fact, _, _, _ = txtfact_to_vector(txt_fact, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "        kg_vectors.append(v_fact)\n",
    "    kg_vector = np.zeros(300)\n",
    "    for vector in kg_vectors:\n",
    "        kg_vector = kg_vector + vector\n",
    "        #kg_vector = uv.icds_composition(kg_vector, vector)\n",
    "    return(kg_vector)\n",
    "\n",
    "# Receives a pair of knowldege graphs (two sentences) and returns a similarity measure between the two sentences\n",
    "def txtpair_sim(kg_pair, stop_words=True, punct_marks=False, embed_model='w2v', mu='ratio'):     \n",
    "    sims = []\n",
    "    kg0 = kg_pair[0]\n",
    "    kg1 = kg_pair[1]\n",
    "    kg0_vector = txtkg_to_vector(kg0, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "    kg1_vector = txtkg_to_vector(kg1, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu)\n",
    "    sim = max(0, uv.cos_sim(kg0_vector, kg1_vector))  \n",
    "    #sim = uv.icm_sim(kg0_vector, kg1_vector, beta=1.2)     # icm sim \n",
    "    #sim = kg0_vector @ kg1_vector                          # dot sim\n",
    "    #sim = np.linalg.norm(kg0_vector - kg1_vector)          # eucl sim\n",
    "    return(sim)\n",
    "\n",
    "# Correlation with trues in a dataset of KG pairs\n",
    "def txt_kgs_sim(ds_txt_pairs, true_scores, stop_words=True, punct_marks=False, embed_model='w2v', mu='ratio'):\n",
    "    sims = []\n",
    "    for pair in ds_txt_pairs:\n",
    "        sims.append(txtpair_sim(pair, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual baselines by facts (weights = (1., 1., 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr. for MSRpar: 0.4889874832385299 \n",
      "\n",
      "Corr. for MSRvid: 0.8138064009323206 \n",
      "\n",
      "Corr. for MSRanswer: 0.428785711078538 \n",
      "\n",
      "Corr. for def2def: 0.49104606918558713 \n",
      "\n",
      "Corr. for def2def_adj: 0.5534544019433109 \n",
      "\n",
      "Corr. for def2def250: 0.5880439602268333 \n",
      "\n",
      "\tCorrelations mean: 0.56068733776752\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "names = ['MSRpar', 'MSRvid', 'MSRanswer', 'def2def', 'def2def_adj', 'def2def250']\n",
    "sets = [par_samples, vid_samples, answer_samples, def2def_samples, def2def_adj_samples, def2def250_samples]\n",
    "true_scores = [par_scores, vid_scores, answer_scores, def2def_scores, def2def_adj_scores, def2def250_scores]\n",
    "correlations = []\n",
    "\n",
    "# Test combinations: embedding = ('w2v', 'glove'); mu = (0, 1, 'ratio')\n",
    "for idx, elem in enumerate(sets):\n",
    "    scores = true_scores[idx]\n",
    "    corr, sims = txt_kgs_sim(elem, scores, stop_words=True, \n",
    "                               punct_marks=False, embed_model='w2v', mu='ratio')   \n",
    "    correlations.append(corr)\n",
    "    #print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "    #print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "    print('Corr. for', names[idx]+':', corr, '\\n')\n",
    "print('\\tCorrelations mean:', np.mean(np.array(correlations)))\n",
    "\n",
    "#paper_refs = [0.42, 0.82, 0.52, 0.52] # -> mean = .57 | W2V + BEST STR + F.INF + COS\n",
    "# This file: ok 20250318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unified baseline by facts: with original DEF2DEF (weights = (1., 1., 3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified dataset size: 3747\n"
     ]
    }
   ],
   "source": [
    "# New unified datasets: 80% of samples and 83% of triplets are from DEF2DEF if original dataset \n",
    "samples = par_samples + vid_samples + answer_samples + def2def_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def_scores]   # def2def scores 0-50 -> 0-5\n",
    "print('Unified dataset size:', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.5230496397117688 1.2987121599071274\n",
      "Sim scores min, max, mean and std: 0.0 5.0000000000000036 1.8913147753614241 1.1798341943056625\n",
      "Correlation: 0.4363632343040948\n"
     ]
    }
   ],
   "source": [
    "corr, sims = txt_kgs_sim(samples, scores, stop_words=True, punct_marks=True, embed_model='w2v', mu=1)   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unified baseline by facts: with adjusted DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified dataset size: 3044\n"
     ]
    }
   ],
   "source": [
    "samples = par_samples + vid_samples + answer_samples + def2def_adj_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def_adj_scores]   # def2def scores 0-50 -> 0-5\n",
    "print('Unified dataset size:', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.4978209592641263 1.3166309708785562\n",
      "Sim scores min, max, mean and std: 0.0 5.0000000000000036 2.025512038318277 1.224489238164242\n",
      "Correlation: 0.47984201478817323\n"
     ]
    }
   ],
   "source": [
    "corr, sims = txt_kgs_sim(samples, scores, stop_words=True, punct_marks=True, embed_model='w2v', mu='ratio')   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unified baseline by facts: with adjusted DEF2DEF_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified dataset size: 1004\n"
     ]
    }
   ],
   "source": [
    "samples = par_samples + vid_samples + answer_samples + def2def250_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def250_scores]   # def2def scores 0-50 -> 0-5\n",
    "print('Unified dataset size:', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.550066733067729 1.4702609203245356\n",
      "Sim scores min, max, mean and std: 0.0 5.0000000000000036 2.8584363034534332 1.4156776341649313\n",
      "Correlation: 0.5405930536972031\n"
     ]
    }
   ],
   "source": [
    "corr, sims = txt_kgs_sim(samples, scores, stop_words=True, punct_marks=True, embed_model='w2v', mu='ratio')   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unified correlations summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code: ok 20250615\n",
    "\n",
    "# Individual baselines      # weights = 1., 1., 2. | sw = True\n",
    "# Corr. for MSRpar: 0.4889874832385299 \n",
    "# Corr. for MSRvid: 0.8138064009323206 \n",
    "# Corr. for MSRanswer: 0.428785711078538 \n",
    "# Corr. for def2def: 0.49104606918558713 \n",
    "#       Mean four original datasets: 0.556\n",
    "# Corr. for def2def_adj: 0.5534544019433109 \n",
    "# Corr. for def2def250: 0.5880439602268333 \n",
    "\n",
    "# Unified datasets          # weights = 1., 1., 3. | sw = True\n",
    "# With original DEF2DEF = 0.436     \n",
    "# With DEF2DEF adjusted = 0.480    \n",
    "# With DEF2DEF_250 adj. = 0.541     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
