{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports and datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "from utils import utils_vectorize as uv\n",
    "\n",
    "with open('datasets/msrpar_samples.pkl', 'rb') as f: \n",
    "    msrpar_samples, msrpar_gold_scores = pickle.load(f)\n",
    "with open('datasets/msrvid_samples.pkl', 'rb') as f: \n",
    "    msrvid_samples, msrvid_gold_scores = pickle.load(f)\n",
    "with open('datasets/msranswer_samples.pkl', 'rb') as f: \n",
    "    msranswer_samples, msranswer_gold_scores = pickle.load(f)\n",
    "# DEF2DEF flavors\n",
    "with open('datasets/def2def_samples.pkl', 'rb') as f: \n",
    "    def2def_samples, def2def_gold_scores = pickle.load(f)\n",
    "with open('datasets/def2def_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def_adjusted_samples, def2def_adjusted_scores = pickle.load(f)\n",
    "with open('datasets/def2def250_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def250_samples, def2def250_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns embedding of an input sentence expressed as a knowledge graph \n",
    "def fact_to_vector(fact:tuple, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):\n",
    "    # 1. A sequential composition into each element of triplet\n",
    "    v_subj = uv.icds_vectorize(fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[0] \n",
    "    v_rel = uv.icds_vectorize(fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[1] \n",
    "    v_obj = uv.icds_vectorize(fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[2] \n",
    "    # 2. A full composition on whole triplet\n",
    "    #v_fact = v_subj + v_rel + v_obj\n",
    "    v_subj_rel = uv.icds_composition(v_subj, v_rel)\n",
    "    v_subj_obj = uv.icds_composition(v_subj, v_obj)\n",
    "    v_rel_obj = uv.icds_composition(v_rel, v_obj)\n",
    "    #v_fact = uv.icds_composition(v_subj_rel, v_obj)   \n",
    "    v_fact = uv.icds_composition(v_subj, v_rel_obj)    \n",
    "    #v_fact = uv.icds_composition(v_subj_obj, v_rel) \n",
    "    return(v_fact, v_subj, v_rel, v_obj)\n",
    "\n",
    "# Returns a new, context embedding with self-attention, if requested\n",
    "def kgtxt_to_selfatt_vectors(txt_kg, stop_words=False, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):\n",
    "    n_facts = len(txt_kg)\n",
    "    weight_mtrx = np.empty((n_facts, n_facts))\n",
    "    v_kg = []\n",
    "    for txt_fact in txt_kg:\n",
    "        v_fact, _, _, _ = fact_to_vector(txt_fact, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        v_kg.append(v_fact)\n",
    "    # 1. Compute fact-wise similarity (self-attention scores)\n",
    "    for idx in range(n_facts):\n",
    "        norm0 = np.max([1.e-125, np.linalg.norm(v_kg[idx])])\n",
    "        for jdx in range(n_facts):\n",
    "            norm1 = np.max([1.e-125, np.linalg.norm(v_kg[jdx])])\n",
    "            norms_ratio = max(1.e-125, min(norm0, norm1)/max(norm0, norm1))  \n",
    "            # Joint information content = IC(x,y) = IC(x) + IC(y) - <x,y>. Here ICM like dot product, with beta = 1.01  \n",
    "            #weight_mtrx[idx][jdx] = (norm0**2 + norm1**2 - v_kg[idx] @ v_kg[jdx])  #uv.cos_sim(v_kg[idx], v_kg[jdx])) \n",
    "            weight_mtrx[idx][jdx] = (norms_ratio**2) * uv.cos_sim(v_kg[idx], v_kg[jdx])  \n",
    "            #weight_mtrx[idx][jdx] = norms_ratio \n",
    "    # 2. Softmax/normalizing (self-att weights)\n",
    "    #norm_weight_mtrx = softmax(weight_mtrx, axis=1)\n",
    "    #norm_weight_mtrx = np.apply_along_axis(np.tanh, axis=1, arr=weight_mtrx) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(uv.sigmoid, axis=1, arr=weight_mtrx) \n",
    "    norm_weight_mtrx = weight_mtrx \n",
    "    # 3. Context vectors (new contextual embeddings)\n",
    "    self_att_mtrx = norm_weight_mtrx @ np.array(v_kg)     \n",
    "    #print(self_att_mtrx)\n",
    "    return(self_att_mtrx, np.array(v_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes similarity between two sentences expressed as knowledge graphs; uses self-attention if requested\n",
    "def pair_sim(kg_pair, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):      # List of tuples of 3 strings\n",
    "    #sims = []\n",
    "    kg0 = kg_pair[0]\n",
    "    kg1 = kg_pair[1]\n",
    "    n_facts_kg0 = len(kg0)\n",
    "    n_facts_kg1 = len(kg1)\n",
    "    n_sims = n_facts_kg0 * n_facts_kg1\n",
    "    sim_mtrx = np.empty((n_facts_kg0, n_facts_kg1))\n",
    "    if self_att:\n",
    "        self_att_mtrx0, _ = kgtxt_to_selfatt_vectors(kg0, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        self_att_mtrx1, _ = kgtxt_to_selfatt_vectors(kg1, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        for idx in range(n_facts_kg0):\n",
    "            norm0 = np.max([1.e-125, np.linalg.norm(self_att_mtrx0[idx])])\n",
    "            for jdx in range(n_facts_kg1):\n",
    "                norm1 = np.max([1.e-125, np.linalg.norm(self_att_mtrx1[jdx])])\n",
    "                norms_weight = np.max(max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1])))\n",
    "                ic_01 = (abs(norm0 + norm1 - self_att_mtrx0[idx] @ self_att_mtrx1[jdx])) \n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx])) \n",
    "                #sim_mtrx[idx][jdx] = uv.icm_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx], beta=beta) \n",
    "                #sim_mtrx[idx][jdx] = np.linalg.norm(self_att_mtrx0[idx] - self_att_mtrx1[jdx])          \n",
    "                #sim_mtrx[idx][jdx] = self_att_mtrx0[idx] @ self_att_mtrx1[jdx].T \n",
    "    else:           # No self-attention\n",
    "        for idx, fact0 in enumerate(kg0):\n",
    "            fact0_vector, _, _, _ = fact_to_vector(fact0, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "            norm0 = np.linalg.norm(fact0_vector)\n",
    "            for jdx, fact1 in enumerate(kg1):\n",
    "                fact1_vector, _, _, _ = fact_to_vector(fact1, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "                norm1 = np.linalg.norm(fact1_vector)\n",
    "                ic_01 = (abs(norm0 + norm1 - fact0_vector @ fact1_vector)) \n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(fact0_vector, fact1_vector)) \n",
    "                #sim_mtrx[idx][jdx] = uv.icm_sim(fact0_vector, fact1_vector, beta=1.4) \n",
    "                #sim_mtrx[idx][jdx] = np.linalg.norm(fact0_vector - fact1_vector)\n",
    "                #sim_mtrx[idx][jdx] = fact0_vector @ fact1_vector\n",
    "    #print(sim_mtrx)\n",
    "    #sents_sim = (np.mean(sim_mtrx)) \n",
    "    sents_sim = (uv.bidir_avgmax_sim(sim_mtrx, stdst='mean')) \n",
    "    #sents_sim = uv.bertscore(sim_mtrx)\n",
    "    return(sents_sim)\n",
    "\n",
    "# Receives a dataset, calls necessary functions, and returns a list of correlations between true and predicted similarities\n",
    "def ds_sents_sim(ds, true_scores, self_att=True, stop_words=True, punct_marks=False, \n",
    "                 beta=1.2, embed_model='w2v', mu='ratio', fact_elems_weights=(1., 1., 2.)):\n",
    "    sims = []\n",
    "    for pair in ds:\n",
    "        sims.append(pair_sim(pair, self_att=self_att, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=fact_elems_weights))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With original DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.5 5.0 3.21282 0.8924830192222147\n",
      "Sim scores min, max, mean and std: 0.4078647181677021 4.986520309295598 3.9676966419359867 0.8177732215403202\n",
      "Corr. for MSRpar: 0.5824865187312448 \n",
      "\n",
      "True scores min, max, mean and std: 0.0 5.0 2.105848 1.6065765144853823\n",
      "Sim scores min, max, mean and std: 0.8359113421734456 5.0 3.341124561591591 1.0528415365016415\n",
      "Corr. for MSRvid: 0.8152805306709233 \n",
      "\n",
      "True scores min, max, mean and std: 0.0 5.0 2.4921259842519685 1.7473086751273263\n",
      "Sim scores min, max, mean and std: 1.4490575243077282 5.000000000000001 4.100845519618307 0.7114536349511864\n",
      "Corr. for MSRanswer: 0.49090158401663037 \n",
      "\n",
      "True scores min, max, mean and std: 0 50 25.029067824924823 12.310826756521202\n",
      "Sim scores min, max, mean and std: 0.0 5.0 1.9746216313983886 0.7557850736923761\n",
      "Corr. for def2def: 0.48004830664207027 \n",
      "\n",
      "Correlations mean: 0.5921792350152172\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a set of labeled datasets and gives additional info \n",
    "names = ['MSRpar', 'MSRvid', 'MSRanswer', 'def2def']\n",
    "sets = [msrpar_samples, msrvid_samples, msranswer_samples, def2def_samples]\n",
    "true_scores = [msrpar_gold_scores, msrvid_gold_scores, msranswer_gold_scores, def2def_gold_scores]\n",
    "correlations = []\n",
    "\n",
    "for idx, elem in enumerate(sets):\n",
    "    scores = true_scores[idx]\n",
    "    corr, sims = ds_sents_sim(elem, scores, self_att=True, stop_words=False, punct_marks=False, \n",
    "                              beta=1.5, embed_model='w2v', mu=1, fact_elems_weights=(1.0, 1.0, 1.9))   \n",
    "    correlations.append(corr)\n",
    "    print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "    print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "    print('Corr. for', names[idx]+':', corr, '\\n')\n",
    "print('Correlations mean:', np.mean(np.array(correlations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file: ok 20250317\n",
    "ALL_no_self_att = [.613]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 1.9 | mu=1 | v_fact=subj·rel_obj\n",
    "ALL_self_att    = [.593]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 2.2 | mu=1 | v_fact=subj·rel_obj      \n",
    "\n",
    "paper_refs =       [0.42, 0.82, 0.52, 0.53] # -> mean = 0.57 | W2V + BEST STR + F.INF + COS\n",
    "bline_sents =      [0.44, 0.77, 0.46, 0.48] # -> mean = 0.54\n",
    "bline_facts =      [0.51, 0.82, 0.42, 0.49] # -> mean = 0.56\n",
    "dsets_best_corrs = [0.63, 0.83, 0.59, 0.51] # -> mean = 0.64  \n",
    "# This file: best results (no self-att) resume for datasets:\n",
    "model_mean_corrs = [0.61, 0.81, 0.55, 0.48] # -> mean = 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With adjusted DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.5 5.0 3.21282 0.8924830192222147\n",
      "Sim scores min, max, mean and std: 0.4263387526907665 4.987039938268274 3.9623629223121317 0.818220326385093\n",
      "Corr. for MSRpar: 0.5856849078833871 \n",
      "\n",
      "True scores min, max, mean and std: 0.0 5.0 2.105848 1.6065765144853823\n",
      "Sim scores min, max, mean and std: 0.7821638518421423 5.0 3.319480636813137 1.0752708342034265\n",
      "Corr. for MSRvid: 0.8145607140596144 \n",
      "\n",
      "True scores min, max, mean and std: 0.0 5.0 2.4921259842519685 1.7473086751273263\n",
      "Sim scores min, max, mean and std: 1.4180754665802828 5.0 4.083602520840214 0.7291004683030337\n",
      "Corr. for MSRanswer: 0.48891924050713587 \n",
      "\n",
      "True scores min, max, mean and std: 0 50 24.631877729257642 12.341513586651518\n",
      "Sim scores min, max, mean and std: 0.08239538384240767 5.000000000000001 2.0318795664987066 0.7659579782203524\n",
      "Corr. for def2def: 0.5554186043373748 \n",
      "\n",
      "Correlations mean: 0.6111458666968781\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a set of labeled datasets and gives additional info \n",
    "names = ['MSRpar', 'MSRvid', 'MSRanswer', 'def2def']\n",
    "sets = [msrpar_samples, msrvid_samples, msranswer_samples, def2def_adjusted_samples]\n",
    "true_scores = [msrpar_gold_scores, msrvid_gold_scores, msranswer_gold_scores, def2def_adjusted_scores]\n",
    "correlations = []\n",
    "\n",
    "for idx, elem in enumerate(sets):\n",
    "    scores = true_scores[idx]\n",
    "    corr, sims = ds_sents_sim(elem, scores, self_att=True, stop_words=False, punct_marks=False, \n",
    "                              beta=1.5, embed_model='w2v', mu=1, fact_elems_weights=(1.0, 1.0, 2.0))   \n",
    "    correlations.append(corr)\n",
    "    print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "    print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "    print('Corr. for', names[idx]+':', corr, '\\n')\n",
    "print('Correlations mean:', np.mean(np.array(correlations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file with new DEF2DEF adjusted subset: ok 20250331\n",
    "ALL_no_self_att = [.630]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 1.6 | mu=1 | v_fact=subj·rel_obj\n",
    "corrs = [0.60, 0.81, 0.56, 0.55] # -> mean = 0.63\n",
    "\n",
    "ALL_self_att    = [.611]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.1 1.0 2.0 | mu=1 | v_fact=subj·rel_obj \n",
    "corrs = [0.59, 0.81, 0.49, 0.55] # -> mean = 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DEF2DEF_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.5 5.0 3.21282 0.8924830192222147\n",
      "Sim scores min, max, mean and std: 0.3953972235004608 4.986428620129196 3.9610215694670896 0.8213104102539583\n",
      "Corr. for MSRpar: 0.5840683476095786 \n",
      "\n",
      "True scores min, max, mean and std: 0.0 5.0 2.105848 1.6065765144853823\n",
      "Sim scores min, max, mean and std: 0.8162183826021784 5.000000000000001 3.329364357979125 1.0738371942397933\n",
      "Corr. for MSRvid: 0.8135236249961558 \n",
      "\n",
      "True scores min, max, mean and std: 0.0 5.0 2.4921259842519685 1.7473086751273263\n",
      "Sim scores min, max, mean and std: 1.3849142672543637 5.0 4.080324144879368 0.7333532304696779\n",
      "Corr. for MSRanswer: 0.4873022171067613 \n",
      "\n",
      "True scores min, max, mean and std: 1 49 23.904 12.394465861827205\n",
      "Sim scores min, max, mean and std: 0.40222865687403736 5.0 1.983367053435215 0.761786920221834\n",
      "Corr. for def2def: 0.5775524074900298 \n",
      "\n",
      "Correlations mean: 0.6156116493006314\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a set of labeled datasets and gives additional info \n",
    "names = ['MSRpar', 'MSRvid', 'MSRanswer', 'def2def']\n",
    "sets = [msrpar_samples, msrvid_samples, msranswer_samples, def2def250_samples]\n",
    "true_scores = [msrpar_gold_scores, msrvid_gold_scores, msranswer_gold_scores, def2def250_scores]\n",
    "correlations = []\n",
    "\n",
    "for idx, elem in enumerate(sets):\n",
    "    scores = true_scores[idx]\n",
    "    corr, sims = ds_sents_sim(elem, scores, self_att=True, stop_words=False, punct_marks=False, \n",
    "                              beta=1.5, embed_model='w2v', mu=1, fact_elems_weights=(1.1, 1.0, 2.1))   \n",
    "    correlations.append(corr)\n",
    "    print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "    print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "    print('Corr. for', names[idx]+':', corr, '\\n')\n",
    "print('Correlations mean:', np.mean(np.array(correlations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file with new DEF2DEF adjusted subset and _250_ samples: ok 20250331\n",
    "ALL_no_self_att = [.638]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 0.9 1.0 1.8 | mu=1 | v_fact=subj·rel_obj\n",
    "corrs = [0.61, 0.81, 0.56, 0.57] # -> mean = 0.64\n",
    "\n",
    "ALL_self_att    = [.616]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.1 1.0 2.1 | mu=1 | v_fact=subj·rel_obj \n",
    "corrs = [0.58, 0.81, 0.49, 0.58] # -> mean = 0.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With original DEF2DEF     | This file: ok 20250616\n",
    "ALL_no_self_att = [.613]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 1.9 | mu=1 | v_fact=subj·rel_obj\n",
    "model_mean_corrs = [0.61, 0.81, 0.55, 0.48] # -> mean = 0.61\n",
    "ALL_self_att    = [.593]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 2.2 | mu=1 | v_fact=subj·rel_obj      \n",
    "self_att_corrs = [0.58, 0.82, 0.49, 0.48] # -> mean = 0.59\n",
    "\n",
    "# With DEF2DEF adjusted     | ok 20250616\n",
    "ALL_no_self_att = [.630]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 1.6 | mu=1 | v_fact=subj·rel_obj\n",
    "no_self_att_corrs = [0.60, 0.81, 0.56, 0.55] # -> mean = 0.63\n",
    "ALL_self_att    = [.611]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.1 1.0 2.0 | mu=1 | v_fact=subj·rel_obj \n",
    "self_att_corrs = [0.59, 0.81, 0.49, 0.55] # -> mean = 0.61\n",
    "\n",
    "# With new DEF2DEF _250     | ok 20250616\n",
    "ALL_no_self_att = [.638]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 0.9 1.0 1.8 | mu=1 | v_fact=subj·rel_obj\n",
    "no_self_att_corrs = [0.61, 0.81, 0.56, 0.57] # -> mean = 0.64\n",
    "ALL_self_att    = [.616]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.1 1.0 2.1 | mu=1 | v_fact=subj·rel_obj \n",
    "self_att_corrs = [0.58, 0.81, 0.49, 0.58] # -> mean = 0.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some statistics from original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of sentences:\n",
      "\t [250, 250, 254, 2993]\n",
      "Total pairs:\n",
      "\t 3747\n",
      "Triplets:\n",
      "\t [2402, 657, 998, 19770]\n",
      "Total triplets:\n",
      "\t 23827\n",
      "Triplets by sent:\n",
      "\t [4.8, 1.31, 1.96, 3.3]\n",
      "Words:\n",
      "\t [13486, 2872, 5263, 81556]\n",
      "Vocab (unique words):\n",
      "\t [2724, 565, 618, 2393]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "triplets = []\n",
    "words = []\n",
    "vocab = []\n",
    "triplets_x_sent = []\n",
    "\n",
    "for elem in sets:\n",
    "    n_pairs = 0\n",
    "    n_triplets = 0\n",
    "    n_words = 0\n",
    "    unique_words = set()\n",
    "    for pair in elem:\n",
    "        n_pairs += 1\n",
    "        for sentence in pair:\n",
    "            for triplet in sentence:\n",
    "                utterance = ' '.join(triplet).split()\n",
    "                n_words += len(utterance)\n",
    "                for word in utterance:\n",
    "                    unique_words.add(word)\n",
    "                n_triplets += 1\n",
    "    pairs.append(n_pairs)\n",
    "    triplets.append(n_triplets)\n",
    "    words.append(n_words)\n",
    "    vocab.append(len(unique_words))\n",
    "    triplets_x_sent.append(float('%.2f' %(n_triplets /(n_pairs*2))))\n",
    "\n",
    "print('Pairs of sentences:\\n\\t', pairs)\n",
    "print('Total pairs:\\n\\t', sum(pairs))\n",
    "print('Triplets:\\n\\t', triplets)\n",
    "print('Total triplets:\\n\\t', sum(triplets))\n",
    "print('Triplets by sent:\\n\\t', triplets_x_sent)\n",
    "print('Words:\\n\\t', words)\n",
    "print('Vocab (unique words):\\n\\t', vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
