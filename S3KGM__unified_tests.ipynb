{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports and datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "from utils import utils_vectorize as uv\n",
    "\n",
    "with open('datasets/msrpar_samples.pkl', 'rb') as f: \n",
    "    par_samples, par_scores = pickle.load(f)\n",
    "with open('datasets/msrvid_samples.pkl', 'rb') as f: \n",
    "    vid_samples, vid_scores = pickle.load(f)\n",
    "with open('datasets/msranswer_samples.pkl', 'rb') as f: \n",
    "    answer_samples, answer_scores = pickle.load(f)\n",
    "\n",
    "# DEF2DEF flavors\n",
    "with open('datasets/def2def_samples.pkl', 'rb') as f: \n",
    "    def2def_samples, def2def_scores = pickle.load(f)\n",
    "with open('datasets/def2def_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def_adjusted_samples, def2def_adjusted_scores = pickle.load(f)\n",
    "with open('datasets/def2def250_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def250_samples, def2def250_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives embedding of a sentence expressed as a knowledge graph \n",
    "def fact_to_vector(fact:tuple, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 2.0)):\n",
    "    # 1. A sequential composition into each element of triplet\n",
    "    v_subj = uv.icds_vectorize(fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[0] #*.5\n",
    "    v_rel = uv.icds_vectorize(fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[1] #*1.1 \n",
    "    v_obj = uv.icds_vectorize(fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[2] # *2.4\n",
    "    # 2. A full composition on whole triplet\n",
    "    #v_fact = v_subj + v_rel + v_obj\n",
    "    v_subj_rel = uv.icds_composition(v_subj, v_rel)\n",
    "    v_subj_obj = uv.icds_composition(v_subj, v_obj)\n",
    "    v_rel_obj = uv.icds_composition(v_rel, v_obj)\n",
    "    v_fact = uv.icds_composition(v_subj_rel, v_obj)    \n",
    "    #v_fact = uv.icds_composition(v_subj, v_rel_obj)    \n",
    "    #v_fact = uv.icds_composition(v_subj_obj, v_rel) \n",
    "    return(v_fact, v_subj, v_rel, v_obj)\n",
    "\n",
    "# Returns new, context embeddings with self-attention, if requested\n",
    "def kgtxt_to_selfatt_vectors(txt_kg, stop_words=False, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 2.0)):\n",
    "    n_facts = len(txt_kg)\n",
    "    weight_mtrx = np.empty((n_facts, n_facts))\n",
    "    v_kg = []\n",
    "    for txt_fact in txt_kg:\n",
    "        v_fact, _, _, _ = fact_to_vector(txt_fact, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        v_kg.append(v_fact)\n",
    "    # 1. Compute fact-wise similarity (self-attention scores)\n",
    "    for idx in range(n_facts):\n",
    "        norm0 = np.max([1.e-125, np.linalg.norm(v_kg[idx])])**1\n",
    "        for jdx in range(n_facts):\n",
    "            norm1 = np.max([1.e-125, np.linalg.norm(v_kg[jdx])])**1\n",
    "            norms_ratio = np.max(max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1]))) \n",
    "            # Joint information content = IC(x,y) = IC(x) + IC(y) - <x,y>. Here ICM like dot product, with beta = 1.01  \n",
    "            #weight_mtrx[idx][jdx] = (norm0**2 + norm1**2 - v_kg[idx] @ v_kg[jdx])  #uv.cos_sim(v_kg[idx], v_kg[jdx])) \n",
    "            weight_mtrx[idx][jdx] = (norms_ratio**2) * uv.cos_sim(v_kg[idx], v_kg[jdx])   \n",
    "    # 2. Softmax/normalizing (self-att weights)\n",
    "    #norm_weight_mtrx = softmax(weight_mtrx, axis=1) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(np.tanh, axis=1, arr=weight_mtrx) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(uv.sigmoid, axis=1, arr=weight_mtrx) \n",
    "    norm_weight_mtrx = weight_mtrx \n",
    "    # 3. Context vectors (new contextual embeddings)\n",
    "    self_att_mtrx = norm_weight_mtrx @ np.array(v_kg)     \n",
    "    return(self_att_mtrx, np.array(v_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes similarity between two sentences expressed as knowledge graphs; uses self-attention if requested\n",
    "def pair_sim(kg_pair, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 2.0)):   # kg_pair is a list of tuples of 3 strings\n",
    "    kg0 = kg_pair[0]\n",
    "    kg1 = kg_pair[1]\n",
    "    sim_mtrx = np.empty((len(kg0), len(kg1)))\n",
    "    if self_att:\n",
    "        self_att_mtrx0, _ = kgtxt_to_selfatt_vectors(kg0, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        self_att_mtrx1, _ = kgtxt_to_selfatt_vectors(kg1, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        for idx in range(len(kg0)):\n",
    "            for jdx in range(len(kg1)):\n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx])) \n",
    "    else:          \n",
    "        for idx, fact0 in enumerate(kg0):\n",
    "            fact0_vector, _, _, _ = fact_to_vector(fact0, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "            for jdx, fact1 in enumerate(kg1):\n",
    "                fact1_vector, _, _, _ = fact_to_vector(fact1, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) \n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(fact0_vector, fact1_vector))\n",
    "    #sents_sim = (np.mean(sim_mtrx)) \n",
    "    sents_sim = (uv.bidir_avgmax_sim(sim_mtrx, stdst='mean'))  \n",
    "    #sents_sim = uv.bertscore(sim_mtrx) \n",
    "    return(sents_sim)\n",
    "\n",
    "# Receives a dataset, calls necessary functions, and returns a list of correlations between true and predicted similarities\n",
    "def ds_sents_sim(ds, true_scores, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 2.0)):\n",
    "    sims = []\n",
    "    for pair in ds:\n",
    "        sims.append(pair_sim(pair, self_att=self_att, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With original DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New unified datasets: 80% of samples and 83% of triplets are from DEF2DEF\n",
    "samples = par_samples + vid_samples + answer_samples + def2def_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def_scores]   # def2def scores 0-50 -> 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.5230496397117688 1.2987121599071274\n",
      "Sim scores min, max, mean and std: 0.0 5.000000000000001 1.9410112279557226 1.1643903815883945\n",
      "Correlation: 0.4486059682012903\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a labeled dataset and gives additional info \n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=True,  \n",
    "                        stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(.8, .8, 2.1))   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)\n",
    "\n",
    "# This file: ok 20250615\n",
    "# All datasets correlations\n",
    "#   0.423 No self-att 1.0 1.0 2.1 \n",
    "#   0.449 Self-att    0.8 0.8 2.1 s_words=True mu=ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With adjusted DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = par_samples + vid_samples + answer_samples + def2def_adjusted_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def_adjusted_scores]   # def2def scores 0-50 -> 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.4978209592641263 1.3166309708785562\n",
      "Sim scores min, max, mean and std: 0.0 5.000000000000001 2.0787000873651333 1.2043549524052133\n",
      "Correlation: 0.49572963089479344\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a labeled dataset and gives additional info \n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=True,  \n",
    "                        stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(.8, .8, 2.1))   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)\n",
    "\n",
    "# This file: ok 20250615\n",
    "# All datasets correlations\n",
    "#   0.468 No self-att 1.0 1.0 2.1 \n",
    "#   0.496 Self-att    0.8 0.8 2.1 s_words=True mu=ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DEF2DEF_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = par_samples + vid_samples + answer_samples + def2def250_samples\n",
    "scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def250_scores]   # def2def scores 0-50 -> 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0.0 5.0 2.550066733067729 1.4702609203245356\n",
      "Sim scores min, max, mean and std: 0.0 5.000000000000001 2.8312302216626666 1.4010997353108443\n",
      "Correlation: 0.5679214681146413\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a labeled dataset and gives additional info \n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=True,  \n",
    "                        stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(.7, 1.5, 3.7))   \n",
    "\n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Correlation:', corr)\n",
    "\n",
    "# This file: ok 20250615\n",
    "# All datasets correlations\n",
    "#   0.559 No self-att 0.4 1.6 3.9 \n",
    "#   0.568 Self-att    0.7 1.5 3.7 s_words=True mu=ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file: ok 20250331    \n",
    "# All datasets with original DEF2DEF\n",
    "no_self_att = [0.423]   # sw=True | mu=1 | sents_sim=bidir | SROweights=1.0 1.0 2.1 | v_fact=s_r·o\n",
    "self_att    = [0.449]   # sw=True | mu=1 | sents_sim=bidir | SROweights=0.8 0.8 2.1 | v_fact=s_r·o \n",
    "\n",
    "# All datasets with DEF2DEF adjusted\n",
    "no_self_att = [0.468]   # sw=True | mu=1 | sents_sim=bidir | SROweights=1.0 1.0 2.1 | v_fact=s_r·o\n",
    "self_att    = [0.496]   # sw=True | mu=1 | sents_sim=bidir | SROweights=0.8 0.8 2.1 | v_fact=s_r·o \n",
    "\n",
    "# All datasets with DEF2DEF_250 adjusted \n",
    "no_self_att = [0.559]   # sw=True | mu=0 | sents_sim=bertscore | SROweights=0.4 1.6 3.9 | v_fact=s_r·o\n",
    "self_att    = [0.568]   # sw=True | mu=1 | sents_sim=bertscore | SROweights=0.7 1.5 3.7 | v_fact=s_r·o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
