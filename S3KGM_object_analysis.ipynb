{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "from utils import utils_vectorize as uv\n",
    "\n",
    "with open('datasets/msrpar_samples.pkl', 'rb') as f: \n",
    "    par_samples, par_scores = pickle.load(f)\n",
    "with open('datasets/msrvid_samples.pkl', 'rb') as f: \n",
    "    vid_samples, vid_scores = pickle.load(f)\n",
    "with open('datasets/msranswer_samples.pkl', 'rb') as f: \n",
    "    answer_samples, answer_scores = pickle.load(f)\n",
    "with open('datasets/def2def_adjusted_samples.pkl', 'rb') as f:      # For this purpose, only adjusted DEF2DEF\n",
    "    def2def_adjusted_samples, def2def_adjusted_scores = pickle.load(f)\n",
    "\n",
    "unified_samples = par_samples + vid_samples + answer_samples + def2def_adjusted_samples\n",
    "unified_scores = par_scores + vid_scores + answer_scores + [score/10 for score in def2def_adjusted_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of fact_to_vector() function can be changed to obtain embeddings of only objects, or relations, or subjects, and combinations as v_subj_obj or so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0\n",
    "def fact_to_vector(fact:tuple, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 1.6)):\n",
    "    global s_norm_accum, r_norm_accum, o_norm_accum, s_norm_count, r_norm_count, o_norm_count, experiments\n",
    "    # 1. A sequential composition into each element of triplet\n",
    "    v_subj = uv.icds_vectorize(fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) \n",
    "    v_rel = uv.icds_vectorize(fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) \n",
    "    v_obj = uv.icds_vectorize(fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) \n",
    "    # Gathering some information about norm sizes\n",
    "    s_norm_accum += np.linalg.norm(v_subj)\n",
    "    r_norm_accum += np.linalg.norm(v_rel)\n",
    "    o_norm_accum += np.linalg.norm(v_obj)\n",
    "    if np.linalg.norm(v_obj) > np.linalg.norm(v_subj) and np.linalg.norm(v_obj) > np.linalg.norm(v_rel):\n",
    "        o_norm_count += 1 \n",
    "    elif np.linalg.norm(v_subj) > np.linalg.norm(v_rel):\n",
    "        s_norm_count += 1\n",
    "    else:\n",
    "        r_norm_count += 1\n",
    "    experiments += 1\n",
    "    # 2. Weighting elements\n",
    "    v_subj = v_subj * weights[0]\n",
    "    v_rel = v_rel * weights[1]\n",
    "    v_obj = v_obj * weights[2]\n",
    "    #v_subj = v_subj * np.linalg.norm(v_subj)\n",
    "    #v_rel = v_rel * np.linalg.norm(v_rel)\n",
    "    #v_obj = v_obj * np.linalg.norm(v_obj) * 1.2\n",
    "    # 3. A full composition on whole triplet\n",
    "    #v_fact = v_subj + v_rel + v_obj\n",
    "    v_subj_rel = uv.icds_composition(v_subj, v_rel)\n",
    "    v_subj_obj = uv.icds_composition(v_subj, v_obj)\n",
    "    v_rel_obj = uv.icds_composition(v_rel, v_obj)\n",
    "    #v_fact = uv.icds_composition(v_subj_rel, v_obj) \n",
    "    v_fact = uv.icds_composition(v_subj, v_rel_obj) \n",
    "    #v_fact = uv.icds_composition(v_subj_obj, v_rel)  \n",
    "    # In return: change v_fact to, for instance, v_obj; or v_subj_obj...\n",
    "    return(v_fact, v_subj, v_rel, v_obj)\n",
    "\n",
    "# Self-attention\n",
    "def kgtxt_to_selfatt_vectors(txt_kg, stop_words=False, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 1.6)):\n",
    "    n_facts = len(txt_kg)\n",
    "    weight_mtrx = np.empty((n_facts, n_facts))\n",
    "    v_kg = []\n",
    "    for txt_fact in txt_kg:\n",
    "        v_fact, _, _, _ = fact_to_vector(txt_fact, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        v_kg.append(v_fact)\n",
    "    # 1. Compute fact-wise similarity (self-attention scores)\n",
    "    for idx in range(n_facts):\n",
    "        norm0 = np.max([1.e-125, np.linalg.norm(v_kg[idx])])\n",
    "        for jdx in range(n_facts):\n",
    "            norm1 = np.max([1.e-125, np.linalg.norm(v_kg[jdx])])\n",
    "            norms_ratio = np.max(max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1]))) \n",
    "            weight_mtrx[idx][jdx] = (norms_ratio ** 2) * uv.cos_sim(v_kg[idx], v_kg[jdx])\n",
    "    # 2. Softmax/normalizing (self-att weights)\n",
    "    norm_weight_mtrx = weight_mtrx \n",
    "    # 3. Context vectors (new contextual embeddings)\n",
    "    self_att_mtrx = norm_weight_mtrx @ np.array(v_kg) \n",
    "    #print(self_att_mtrx)\n",
    "    return(self_att_mtrx, np.array(v_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get similarity between two KGs\n",
    "# Combinations: composition = (sum | ICDS); sim = (cos | ICM | euclid | dot prod); sents_sim: (mean | median | bidir | bertscore)\n",
    "def pair_sim(kg_pair, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 1.6)):  # List of tuples of 3 strings\n",
    "    sims = []\n",
    "    kg0 = kg_pair[0]\n",
    "    kg1 = kg_pair[1]\n",
    "    n_facts_kg0 = len(kg0)\n",
    "    n_facts_kg1 = len(kg1)\n",
    "    n_sims = n_facts_kg0 * n_facts_kg1\n",
    "    sim_mtrx = np.empty((n_facts_kg0, n_facts_kg1))\n",
    "    if self_att:\n",
    "        self_att_mtrx0, _ = kgtxt_to_selfatt_vectors(kg0, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        self_att_mtrx1, _ = kgtxt_to_selfatt_vectors(kg1, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        for idx in range(n_facts_kg0):\n",
    "            norm0 = np.max([1.e-125, np.linalg.norm(self_att_mtrx0[idx])])\n",
    "            for jdx in range(n_facts_kg1):\n",
    "                norm1 = np.max([1.e-125, np.linalg.norm(self_att_mtrx1[jdx])])\n",
    "                norms_weight = np.max(max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1])))\n",
    "                ic_01 = (abs(norm0 + norm1 - self_att_mtrx0[idx] @ self_att_mtrx1[jdx])) \n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx])) \n",
    "    else:           # No self-attention\n",
    "        for idx, fact0 in enumerate(kg0):\n",
    "            fact0_vector, _, _, _ = fact_to_vector(fact0, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "            norm0 = np.linalg.norm(fact0_vector)\n",
    "            for jdx, fact1 in enumerate(kg1):\n",
    "                fact1_vector, _, _, _ = fact_to_vector(fact1, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "                norm1 = np.linalg.norm(fact1_vector)\n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(fact0_vector, fact1_vector)) \n",
    "    #sents_sim = (np.mean(sim_mtrx)) \n",
    "    sents_sim = uv.bidir_avgmax_sim(sim_mtrx, stdst='mean') \n",
    "    #sents_sim = uv.bertscore(sim_mtrx)\n",
    "    return(sents_sim)\n",
    "\n",
    "# Correlation with trues in a dataset of KG pairs\n",
    "def ds_sents_sim(ds, true_scores, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1.0, 1.0, 1.6)):\n",
    "    sims = []\n",
    "    for pair in ds:\n",
    "        sims.append(pair_sim(pair, self_att=self_att, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu,  weights=weights))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual datasets (each one with best set of parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRpar dataset:\n",
      "Corr. for MSRpar: 0.6130285647577763 \n",
      "\n",
      "4.4839947820369055 2.872382723745743 5.53839483817318 3124 846 3555 7525\n",
      "Percentages of bigger norms: s: 0.41514950166112957 r: 0.11242524916943522 o: 0.47242524916943524\n"
     ]
    }
   ],
   "source": [
    "# MSRpar dataset | Best weights: no sa (0.9, 0.4, 2.9); sa (0.6, 0.3, 3.2) \n",
    "# no_self_att =   [.626]  # s_words no | sim_mtrx=bertscore | SRO ws: 0.9 0.4 2.9 | mu=0 | v_fact=subj·rel_obj | \n",
    "# self_att =      [.619]  # s_words no | sim_mtrx=bertscore | SRO ws: 0.6 0.3 3.2 | mu=0 | v_fact=subj·rel_obj | \n",
    "\n",
    "corr, sims = ds_sents_sim(par_samples, par_scores, self_att=False, \n",
    "                              stop_words=False, punct_marks=False, beta=1.5, embed_model='w2v', mu=0, weights=(.9, .4, 2.9))  \n",
    "\n",
    "print('MSRpar dataset:')\n",
    "#print('True scores min, max, mean and std:',np.min(unified_scores), np.max(unified_scores), np.mean(unified_scores), np.std(unified_scores))\n",
    "#print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for MSRpar:', corr, '\\n')\n",
    "print(s_norm_accum/experiments, r_norm_accum/experiments, o_norm_accum/experiments, s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "print('Percentages of bigger norms: s:', s_norm_count/experiments, 'r:', r_norm_count/experiments, 'o:', o_norm_count/experiments)\n",
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRvid dataset:\n",
      "Corr. for MSRvid: 0.8250672482659904 \n",
      "\n",
      "2.679514014013282 2.610415814419858 2.9301497516322805 110 150 397 657\n",
      "Percentages of bigger norms: s: 0.167427701674277 r: 0.228310502283105 o: 0.604261796042618\n"
     ]
    }
   ],
   "source": [
    "# MSRvid dataset | Best weights: no sa (1., 1., 1.8); sa (1., 1., 1.75) \n",
    "corr, sims = ds_sents_sim(vid_samples, vid_scores, self_att=True, \n",
    "                              stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=0, weights=(1., 1., 1.8))  \n",
    "\n",
    "print('MSRvid dataset:')\n",
    "#print('True scores min, max, mean and std:',np.min(unified_scores), np.max(unified_scores), np.mean(unified_scores), np.std(unified_scores))\n",
    "#print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for MSRvid:', corr, '\\n')\n",
    "print(s_norm_accum/experiments, r_norm_accum/experiments, o_norm_accum/experiments, s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "print('Percentages of bigger norms: s:', s_norm_count/experiments, 'r:', r_norm_count/experiments, 'o:', o_norm_count/experiments)\n",
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer dataset:\n",
      "Corr. for answer: 0.5868373773233031 \n",
      "\n",
      "2.485584754968897 2.6671725611027073 2.8431897694669557 381 532 722 1635\n",
      "Percentages of bigger norms: s: 0.23302752293577983 r: 0.3253822629969419 o: 0.4415902140672783\n"
     ]
    }
   ],
   "source": [
    "# answer dataset | Best weights: no sa (.7, 1.6, 1.6); sa (1., 1.4, 1.2) \n",
    "# no_self_att =   [.593]  # s_words false | sim_mtrx=bidir mean   | SRO ws: 0.7 1.6 1.6 | mu=1 | v_fact=subj_obj·rel\n",
    "# self_att =      [.517]  # s_words false | sim_mtrx=mean         | SRO ws: 1.0 1.4 1.2 | mu=1 | v_fact=subj_rel·obj | sa=no_norm\n",
    "\n",
    "corr, sims = ds_sents_sim(answer_samples, answer_scores, self_att=False, \n",
    "                              stop_words=False, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(.7, 1.6, 1.6))  \n",
    "\n",
    "print('answer dataset:')\n",
    "#print('True scores min, max, mean and std:',np.min(unified_scores), np.max(unified_scores), np.mean(unified_scores), np.std(unified_scores))\n",
    "#print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for answer:', corr, '\\n')\n",
    "print(s_norm_accum/experiments, r_norm_accum/experiments, o_norm_accum/experiments, s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "print('Percentages of bigger norms: s:', s_norm_count/experiments, 'r:', r_norm_count/experiments, 'o:', o_norm_count/experiments)\n",
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEF2DEF adj. dataset:\n",
      "Corr. for DEF2DEF adj.: 0.5840108619084013 \n",
      "\n",
      "2.8419606527590853 1.0367040057166153 3.168220801892817 5783 1099 8312 15194\n",
      "Percentages of bigger norms: s: 0.38061076740818744 r: 0.07233118336185336 o: 0.5470580492299592\n"
     ]
    }
   ],
   "source": [
    "# DEF2DEF adj. dataset | Best weights: no sa (1.1, 0.6, 1.3); sa (1.2, 0.7, 1.65) \n",
    "# no_self_att =   [0.581]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.1 0.6 1.3 | mu=1 | v_fact=subj_rel·obj \n",
    "# self_att =      [0.584]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.2 0.7 1.65 | mu=1 | v_fact=subj_obj·rel \n",
    "corr, sims = ds_sents_sim(def2def_adjusted_samples, def2def_adjusted_scores, self_att=True,\n",
    "                              stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(1.1, 0.6, 1.3))\n",
    "\n",
    "print('DEF2DEF adj. dataset:')\n",
    "#print('True scores min, max, mean and std:',np.min(unified_scores), np.max(unified_scores), np.mean(unified_scores), np.std(unified_scores))\n",
    "#print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for DEF2DEF adj.:', corr, '\\n')\n",
    "print(s_norm_accum/experiments, r_norm_accum/experiments, o_norm_accum/experiments, s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "print('Percentages of bigger norms: s:', s_norm_count/experiments, 'r:', r_norm_count/experiments, 'o:', o_norm_count/experiments)\n",
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint tests (individual datasets with same parmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3199 763 3563 7525\n",
      "Dataset: MSRpar\n",
      "True scores min, max, mean and std: 0.5 5.0 3.21282 0.8924830192222147\n",
      "Sim scores min, max, mean and std: 0.41859224415024243 4.969872247862995 3.7316982898976923 0.7772476149079477\n",
      "Corr. for MSRpar: 0.5903491889627053 \n",
      "\n",
      "3277 1146 3890 8313\n",
      "Dataset: MSRvid\n",
      "True scores min, max, mean and std: 0.0 5.0 2.105848 1.6065765144853823\n",
      "Sim scores min, max, mean and std: 0.9680423140525818 5.000000149011612 3.3478207177669637 0.9679625538404487\n",
      "Corr. for MSRvid: 0.8067548003910293 \n",
      "\n",
      "3658 1678 4612 9948\n",
      "Dataset: MSRanswer\n",
      "True scores min, max, mean and std: 0.0 5.0 2.4921259842519685 1.7473086751273263\n",
      "Sim scores min, max, mean and std: 1.905647440660122 5.000000596046448 4.035044073575274 0.6596854151975906\n",
      "Corr. for MSRanswer: 0.5586727542550838 \n",
      "\n",
      "15717 5868 21641 43226\n",
      "Dataset: def2def_adj\n",
      "True scores min, max, mean and std: 0 50 24.631877729257642 12.341513586651518\n",
      "Sim scores min, max, mean and std: 0.13162139531477904 5.000000298023224 1.8305093865144875 0.6683311185957764\n",
      "Corr. for def2def_adj: 0.5479589813522572 \n",
      "\n",
      "Correlations mean: 0.6259339312402689\n",
      "15717 5868 21641 43226\n",
      "Percentages of bigger norms: s: 0.363600610743534 r: 0.13575163096284643 o: 0.5006477582936196\n"
     ]
    }
   ],
   "source": [
    "# Main: launches computation of similarities correlation from a set of labeled datasets and gives additional info \n",
    "# This file with DEF2DEF adjusted: ok 20250331\n",
    "ALL_no_self_att = [.630]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.0 1.0 1.6 | mu=1 | v_fact=subj·rel_obj\n",
    "ALL_self_att    = [.611]  # s_words=false | sim_mtrx=bidir mean | SRO weights: 1.1 1.0 2.0 | mu=1 | v_fact=subj·rel_obj \n",
    "\n",
    "names = ['MSRpar', 'MSRvid', 'MSRanswer', 'def2def_adj']\n",
    "sets = [par_samples, vid_samples, answer_samples, def2def_adjusted_samples]\n",
    "true_scores = [par_scores, vid_scores, answer_scores, def2def_adjusted_scores]\n",
    "correlations = []\n",
    "\n",
    "for idx, elem in enumerate(sets):\n",
    "    scores = true_scores[idx]\n",
    "    corr, sims = ds_sents_sim(elem, scores, self_att=False, \n",
    "                              stop_words=False, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(1, 1, 1.6))   \n",
    "    print(s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "    correlations.append(corr)\n",
    "    print('Dataset:', names[idx])\n",
    "    print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "    print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "    print('Corr. for', names[idx]+':', corr, '\\n')\n",
    "print('Correlations mean:', np.mean(np.array(correlations)))\n",
    "\n",
    "print(s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "print('Percentages of bigger norms: s:', s_norm_count/experiments, 'r:', r_norm_count/experiments, 'o:', o_norm_count/experiments)\n",
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified dataset:\n",
      "True scores min, max, mean and std: 0.0 5.0 2.4978209592641263 1.3166309708785562\n",
      "Sim scores min, max, mean and std: 0.0 5.000000000000001 2.0994930630866966 1.1991706987956299\n",
      "Corr. for unified dataset: 0.49258080059048603 \n",
      "\n",
      "2.9688061627383324 1.1816499513123866 3.391188019426801 7300 1624 10327 19251\n",
      "Percentages of bigger norms: s: 0.37920108046335255 r: 0.08435925406472392 o: 0.5364396654719236\n"
     ]
    }
   ],
   "source": [
    "corr, sims = ds_sents_sim(unified_samples, unified_scores, self_att=True, \n",
    "                              stop_words=True, punct_marks=False, beta=1.5, embed_model='w2v', mu=1, weights=(1., 1.1, 2.4))  \n",
    "\n",
    "print('Unified dataset:')\n",
    "print('True scores min, max, mean and std:',np.min(unified_scores), np.max(unified_scores), np.mean(unified_scores), np.std(unified_scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for unified dataset:', corr, '\\n')\n",
    "print(s_norm_accum/experiments, r_norm_accum/experiments, o_norm_accum/experiments, s_norm_count, r_norm_count, o_norm_count, experiments)\n",
    "print('Percentages of bigger norms: s:', s_norm_count/experiments, 'r:', r_norm_count/experiments, 'o:', o_norm_count/experiments)\n",
    "s_norm_accum = 0.\n",
    "r_norm_accum = 0.\n",
    "o_norm_accum = 0.\n",
    "s_norm_count = 0\n",
    "r_norm_count = 0\n",
    "o_norm_count = 0\n",
    "experiments = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
