{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "from utils import utils_vectorize as uv\n",
    "\n",
    "with open('datasets/def2def_samples.pkl', 'rb') as f: \n",
    "    def2def_samples, def2def_gold_scores = pickle.load(f)\n",
    "\n",
    "with open('datasets/def2def_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def_adjusted_samples, def2def_adjusted_scores = pickle.load(f)\n",
    "\n",
    "with open('datasets/def2def250_adjusted_samples.pkl', 'rb') as f: \n",
    "    def2def250_samples, def2def250_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_to_vector(fact:tuple, stop_words=False, punct_marks=False, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):\n",
    "    # 1. A sequential composition into each element of triplet\n",
    "    v_subj = uv.icds_vectorize(fact[0], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[0] #* 1.2 #* 1.1 \n",
    "    v_rel = uv.icds_vectorize(fact[1], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[1] #* 0.6 #* .45 \n",
    "    v_obj = uv.icds_vectorize(fact[2], stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu) * weights[2] #* 1.5 #* 1.6 \n",
    "    # 2. A full composition on whole triplet\n",
    "    #v_fact = v_subj + v_rel + v_obj\n",
    "    v_subj_rel = uv.icds_composition(v_subj, v_rel)\n",
    "    v_subj_obj = uv.icds_composition(v_subj, v_obj)\n",
    "    v_rel_obj = uv.icds_composition(v_rel, v_obj)\n",
    "    v_fact = uv.icds_composition(v_subj_rel, v_obj) \n",
    "    #v_fact = uv.icds_composition(v_subj_obj, v_rel) \n",
    "    #v_fact = uv.icds_composition(v_subj, v_rel_obj)   \n",
    "    return(v_fact, v_subj, v_rel, v_obj)\n",
    "\n",
    "# Self-attention\n",
    "def kgtxt_to_selfatt_vectors(txt_kg, stop_words=False, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):\n",
    "    n_facts = len(txt_kg)\n",
    "    weight_mtrx = np.empty((n_facts, n_facts))\n",
    "    v_kg = []\n",
    "    for txt_fact in txt_kg:\n",
    "        v_fact, _, _, _ = fact_to_vector(txt_fact, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        v_kg.append(v_fact)\n",
    "    # 1. Compute fact-wise similarity (self-attention scores)\n",
    "    for idx in range(n_facts):\n",
    "        norm0 = np.max([1.e-125, np.linalg.norm(v_kg[idx])])\n",
    "        for jdx in range(n_facts):\n",
    "            norm1 = np.max([1.e-125, np.linalg.norm(v_kg[jdx])])\n",
    "            norms_ratio = max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1])) \n",
    "            # Joint information content = IC(x,y) = IC(x) + IC(y) - <x,y> \n",
    "            #weight_mtrx[idx][jdx] = norm0**2 + norm1**2 - v_kg[idx] @ v_kg[jdx] #uv.cos_sim(v_kg[idx], v_kg[jdx])) \n",
    "            weight_mtrx[idx][jdx] = (norms_ratio ** 2) * uv.cos_sim(v_kg[idx], v_kg[jdx]) \n",
    "    # 2. Softmax/normalizing (self-att weights)\n",
    "    #norm_weight_mtrx = softmax(weight_mtrx, axis=1) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(np.tanh, axis=1, arr=weight_mtrx) \n",
    "    #norm_weight_mtrx = np.apply_along_axis(uv.sigmoid, axis=1, arr=weight_mtrx) \n",
    "    norm_weight_mtrx = weight_mtrx\n",
    "    # 3. Context vectors (new contextual embeddings)\n",
    "    self_att_mtrx = norm_weight_mtrx @ np.array(v_kg) \n",
    "    #print(self_att_mtrx)\n",
    "    return(self_att_mtrx, np.array(v_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get similarity between two KGs\n",
    "# Combinations: composition = (sum | ICDS); sim = (cos | ICM | euclid | dot prod); sents_sim: (mean | median | bidir | bertscore)\n",
    "def pair_sim(kg_pair, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):  # List of tuples of 3 strings\n",
    "    sims = []\n",
    "    kg0 = kg_pair[0]\n",
    "    kg1 = kg_pair[1]\n",
    "    n_facts_kg0 = len(kg0)\n",
    "    n_facts_kg1 = len(kg1)\n",
    "    n_sims = n_facts_kg0 * n_facts_kg1\n",
    "    sim_mtrx = np.empty((n_facts_kg0, n_facts_kg1))\n",
    "    if self_att:\n",
    "        self_att_mtrx0, _ = kgtxt_to_selfatt_vectors(kg0, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        self_att_mtrx1, _ = kgtxt_to_selfatt_vectors(kg1, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights)\n",
    "        for idx in range(n_facts_kg0):\n",
    "            norm0 = np.max([1.e-125, np.linalg.norm(self_att_mtrx0[idx])])\n",
    "            for jdx in range(n_facts_kg1):\n",
    "                norm1 = np.max([1.e-125, np.linalg.norm(self_att_mtrx1[jdx])])\n",
    "                norms_weight = np.max(max(1.e-125, np.min([norm0, norm1])/np.max([norm0, norm1])))\n",
    "                ic_01 = (abs(norm0 + norm1 - self_att_mtrx0[idx] @ self_att_mtrx1[jdx])) \n",
    "                sim_mtrx[idx][jdx] =  max(0, uv.cos_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx])) \n",
    "                #sim_mtrx[idx][jdx] = uv.icm_sim(self_att_mtrx0[idx], self_att_mtrx1[jdx], beta=beta) \n",
    "                #sim_mtrx[idx][jdx] = np.linalg.norm(self_att_mtrx0[idx] - self_att_mtrx1[jdx])         \n",
    "                #sim_mtrx[idx][jdx] = self_att_mtrx0[idx] @ self_att_mtrx1[jdx].T \n",
    "    else:           # No self-attention\n",
    "        for idx, fact0 in enumerate(kg0):\n",
    "            fact0_vector, _, _, _ = fact_to_vector(fact0, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "            norm0 = np.linalg.norm(fact0_vector)\n",
    "            for jdx, fact1 in enumerate(kg1):\n",
    "                fact1_vector, _, _, _ = fact_to_vector(fact1, stop_words=stop_words, punct_marks=punct_marks, embed_model=embed_model, mu=mu, weights=weights) \n",
    "                norm1 = np.linalg.norm(fact1_vector)\n",
    "                sim_mtrx[idx][jdx] = max(0, uv.cos_sim(fact0_vector, fact1_vector)) \n",
    "                #sim_mtrx[idx][jdx] = uv.icm_sim(fact0_vector, fact1_vector, beta=1.4) \n",
    "    #print(sim_mtrx)\n",
    "    #sents_sim = np.mean(sim_mtrx) \n",
    "    sents_sim = uv.bidir_avgmax_sim(sim_mtrx, stdst='mean') \n",
    "    #sents_sim = uv.bertscore(sim_mtrx)\n",
    "    return(sents_sim)\n",
    "\n",
    "# Correlation with true in a dataset of KG pairs\n",
    "def ds_sents_sim(ds, true_scores, self_att=True, stop_words=True, punct_marks=False, beta=1.2, embed_model='w2v', mu='ratio', weights=(1., 1., 2.)):\n",
    "    sims = []\n",
    "    for pair in ds:\n",
    "        sims.append(pair_sim(pair, self_att=self_att, stop_words=stop_words, punct_marks=punct_marks, beta=beta, embed_model=embed_model, mu=mu, weights=weights))\n",
    "    correlation = np.corrcoef(sims, np.array(true_scores))[0][1]\n",
    "    return(correlation, np.array(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With original DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0 50 25.029067824924823 12.310826756521202\n",
      "Sim scores min, max, mean and std: 0.0 5.00000019868215 1.4328392425011043 0.7484405117155343\n",
      "Corr. for MSRpar: 0.512111440065369 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test combinations: embedding = ('w2v', 'glove'); mu = (0, 1, 'ratio')\n",
    "samples = def2def_samples\n",
    "scores = def2def_gold_scores\n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=False,  \n",
    "                        stop_words=True, punct_marks=True, beta=1.5, embed_model='w2v', mu=1, weights=(1.2, 0.6, 1.5))   \n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for MSRpar:', corr, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results | This file: ok 20250318\n",
    "paper_ref =     [0.53]  # W2V + BEST STR + F.INF + COS\n",
    "no_self_att =   [0.5121]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.2 0.6 1.5  | mu=1 | v_fact=subj_obj·rel\n",
    "self_att =      [0.5141]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.1 0.45 1.6 | mu=1 | v_fact=subj_obj·rel | sa=no_sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With adjusted DEF2DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 0 50 24.631877729257642 12.341513586651518\n",
      "Sim scores min, max, mean and std: 0.0 5.0 1.6909885097834403 0.8561488768689401\n",
      "Corr. for MSRpar: 0.5843654125589703 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test combinations: embedding = ('w2v', 'glove'); mu = (0, 1, 'ratio')\n",
    "samples = def2def_adjusted_samples\n",
    "scores = def2def_adjusted_scores\n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=True,  \n",
    "                        stop_words=True, punct_marks=True, beta=1.5, embed_model='w2v', mu=1, weights=(1.2, 0.7, 1.65))   \n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for MSRpar:', corr, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results with adjusted DEF2DEF | This file: ok 20250327\n",
    "no_self_att =   [0.581]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.1 0.6 1.3 | mu=1 | v_fact=subj_rel·obj \n",
    "self_att =      [0.584]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.2 0.7 1.65 | mu=1 | v_fact=subj_obj·rel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DEF2DEF_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True scores min, max, mean and std: 1 49 23.904 12.394465861827205\n",
      "Sim scores min, max, mean and std: 0.023298098055828866 5.0 1.6291712005364851 0.8461793445885769\n",
      "Corr. for MSRpar: 0.6065150766396528 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test combinations: embedding = ('w2v', 'glove'); mu = (0, 1, 'ratio')\n",
    "samples = def2def250_samples\n",
    "scores = def2def250_scores\n",
    "corr, sims = ds_sents_sim(samples, scores, self_att=True,  \n",
    "                        stop_words=True, punct_marks=True, beta=1.5, embed_model='w2v', mu=1, weights=(1., 0.2, 1.5))   \n",
    "print('True scores min, max, mean and std:',np.min(scores), np.max(scores), np.mean(scores), np.std(scores))\n",
    "print('Sim scores min, max, mean and std:', np.min(sims*5), np.max(sims*5), np.mean(sims*5), np.std(sims*5))\n",
    "print('Corr. for MSRpar:', corr, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results with adjusted DEF2DEF and reduction to _250_ samples | This option: ok 20250329\n",
    "no_self_att =   [0.601]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.1 0.4 1.5 | mu=1 | v_fact=subj_rel·obj \n",
    "self_att =      [0.607]  # s_words ok | sim_mtrx=bidir mean | SRO ws: 1.0 0.2 1.5 | mu=1 | v_fact=subj_rel·obj "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
